{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "362a190c",
   "metadata": {},
   "source": [
    "# Q.6 CNN with SGD, MC Dropout, and Epistemic Uncertainty\n",
    "Train a controlled convolutional neural network (CNN) on a subset of the SVHN dataset using SGD optimizer. Then, apply Monte Carlo (MC) Dropout at inference to estimate both test accuracy and epistemic uncertainty. Set random seeds to 42. Use the following configuration:\n",
    "- Load SVHN and normalize pixel values to [0,1]\n",
    "- Use only the first 2000 training samples and first 500 test samples\n",
    "- Input shape: 32 × 32 × 3\n",
    "- CNN architecture:\n",
    "- Conv2D: 32 filters, 3×3 kernel, ReLU activation\n",
    "- MaxPooling2D: 2×2\n",
    "- Conv2D: 64 filters, 3×3 kernel, ReLU activation\n",
    "- MaxPooling2D: 2×2\n",
    "- Flatten\n",
    "- Dense: 128 neurons, ReLU activation\n",
    "- Dropout: 0.25 (keep during inference for MC Dropout)\n",
    "- Output layer: 10 neurons with softmax\n",
    "- Optimizer: SGD with momentum = 0.9, learning rate = 0.01\n",
    "- Loss: sparse categorical crossentropy\n",
    "- epochs = 15, batch size = 32\n",
    "- For MC Dropout:\n",
    "- Enable dropout during inference\n",
    "- Average predictions over 20 stochastic forward passes\n",
    "- Compute the epistemic uncertainty as the predictive variance across passes\n",
    "\n",
    "Q6.1 Report the plain test accuracy of the CNN trained with SGD (no MC Dropout).\n",
    "\n",
    "Q6.2 Report the MC Dropout-enhanced accuracy (averaging 20 stochastic predictions).\n",
    "\n",
    "Q6.3 Compute the average epistemic uncertainty (mean predictive variance) across all test samples. Report it as a deterministic number rounded to 3 decimal places\n",
    "\n",
    "\n",
    "# Imports, seed and data loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebc95e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torchvision.datasets import SVHN\n",
    "import numpy as np, random\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_full = SVHN('./data', split='train', download=True, transform=transform)\n",
    "test_full = SVHN('./data', split='test', download=True, transform=transform)\n",
    "\n",
    "train_subset = Subset(train_full, range(2000))\n",
    "test_subset = Subset(test_full, range(500))\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_subset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e324f976",
   "metadata": {},
   "source": [
    "# CNN definition\n",
    "(this is gonna be the pre-MC model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd4d3faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 2.2439, Accuracy: 20.20%\n",
      "Epoch 2/15, Loss: 2.2325, Accuracy: 20.40%\n",
      "Epoch 3/15, Loss: 2.2279, Accuracy: 20.40%\n",
      "Epoch 4/15, Loss: 2.2260, Accuracy: 20.40%\n",
      "Epoch 5/15, Loss: 2.2227, Accuracy: 20.40%\n",
      "Epoch 6/15, Loss: 2.2199, Accuracy: 20.40%\n",
      "Epoch 7/15, Loss: 2.2183, Accuracy: 20.40%\n",
      "Epoch 8/15, Loss: 2.2142, Accuracy: 20.40%\n",
      "Epoch 9/15, Loss: 2.2064, Accuracy: 20.40%\n",
      "Epoch 10/15, Loss: 2.1948, Accuracy: 20.40%\n",
      "Epoch 11/15, Loss: 2.1812, Accuracy: 20.30%\n",
      "Epoch 12/15, Loss: 2.1639, Accuracy: 20.40%\n",
      "Epoch 13/15, Loss: 2.1220, Accuracy: 22.80%\n",
      "Epoch 14/15, Loss: 2.0645, Accuracy: 25.20%\n",
      "Epoch 15/15, Loss: 1.9714, Accuracy: 32.30%\n"
     ]
    }
   ],
   "source": [
    "class CNN_SVHN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*8*8, 128), nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.features(x))\n",
    "\n",
    "# Model, optimizer, loss\n",
    "model = CNN_SVHN().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "Epochs = 15\n",
    "for epoch in range(Epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    correct, total = 0, 0  # reset each epoch\n",
    "    \n",
    "    for x, y in train_dl:\n",
    "        x, y = x.to(device), y.to(device).long()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        preds = out.argmax(1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    \n",
    "    acc = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{Epochs}, Loss: {running_loss/len(train_dl):.4f}, Accuracy: {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d1e2c0",
   "metadata": {},
   "source": [
    "### Q6.1 \n",
    "The accuracy is low because the model doesn’t have enough data or depth to really learn the complex patterns in the SVHN images. It’s only trained on 2000 samples, and the network is pretty simple. Also, using plain SGD makes learning slower, so it doesn’t reach good performance within just 15 epochs. ( i think WIP)\n",
    "\n",
    "### Q6.2 Report the MC Dropout-enhanced accuracy (averaging 20 stochastic predictions).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f825375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MC Dropout-enhanced Accuracy: 28.80%\n"
     ]
    }
   ],
   "source": [
    "def mc_dropout_predictions(model, x, passes=20):\n",
    "    \"\"\"MC Dropout predictions: keep dropout active during inference.\"\"\"\n",
    "    model.train()  # ENABLE dropout\n",
    "    preds = []\n",
    "    for _ in range(passes):\n",
    "        preds.append(torch.softmax(model(x), dim=1).unsqueeze(0))\n",
    "    return torch.cat(preds, dim=0)  # [passes, batch, classes]\n",
    "\n",
    "correct_mc, total_mc = 0, 0\n",
    "with torch.no_grad():\n",
    "    for x, y in test_dl:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        preds_mc = mc_dropout_predictions(model, x, passes=20)\n",
    "        mean_preds = preds_mc.mean(0)\n",
    "        final_preds = mean_preds.argmax(1)\n",
    "        correct_mc += (final_preds == y).sum().item()\n",
    "        total_mc += y.size(0)\n",
    "\n",
    "mc_acc = 100 * correct_mc / total_mc\n",
    "print(f\"MC Dropout-enhanced Accuracy: {mc_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da085a8e",
   "metadata": {},
   "source": [
    "## Q6.3 Compute the average epistemic uncertainty (mean predictive variance) across all test samples. Report it as a deterministic number rounded to 3 decimal places\n",
    "\n",
    "### Whats epistemic uncertainty?\n",
    "Epistemic uncertainty is the uncertainty in a model’s predictions due to lack of knowledge or limited training data. It reflects how unsure the model is about its predictions and can be reduced by providing more data or improving the model. \n",
    "\n",
    "In practice, MC Dropout estimates it by measuring variability in predictions across multiple stochastic forward passes. High epistemic uncertainty means the model is less confident, while low uncertainty means it is more confident in its outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c8ca06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Epistemic Uncertainty: 0.0\n",
      "Just showing that its very low:\n",
      "Average Epistemic Uncertainty: 0.0004344634071458131\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "epistemic_vars = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, _ in test_dl:\n",
    "        x = x.to(device)\n",
    "        preds_mc = mc_dropout_predictions(model, x, passes=20)  # [20, batch, 10]\n",
    "        # variance per sample over stochastic passes\n",
    "        var_preds = preds_mc.var(0).mean(1)  # mean over classes\n",
    "        epistemic_vars.extend(var_preds.cpu().numpy())\n",
    "\n",
    "avg_epistemic_uncertainty = np.mean(epistemic_vars)\n",
    "print(f\"Average Epistemic Uncertainty: {round(avg_epistemic_uncertainty, 3)}\")\n",
    "print(\"Just showing that its very low:\")\n",
    "print(f\"Average Epistemic Uncertainty: {round(avg_epistemic_uncertainty, 10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01af09e",
   "metadata": {},
   "source": [
    "The model has a very low epistemic uncertainty, so much so that rounding it up to 3 decimals will only display 0. This means the model is extremely confident in its predictions. but given its low test accuracy, this confidence is likely wrong, the model is consistently making similar predictions, even when they are wrong. This can be because it may be underfitting or not learning new representations from the limited training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
