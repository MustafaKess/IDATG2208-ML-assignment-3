{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "362a190c",
   "metadata": {},
   "source": [
    "# Q.6 CNN with SGD, MC Dropout, and Epistemic Uncertainty\n",
    "Train a controlled convolutional neural network (CNN) on a subset of the SVHN dataset using SGD optimizer. Then, apply Monte Carlo (MC) Dropout at inference to estimate both test accuracy and epistemic uncertainty. Set random seeds to 42. Use the following configuration:\n",
    "- Load SVHN and normalize pixel values to [0,1]\n",
    "- Use only the first 2000 training samples and first 500 test samples\n",
    "- Input shape: 32 × 32 × 3\n",
    "- CNN architecture:\n",
    "- Conv2D: 32 filters, 3×3 kernel, ReLU activation\n",
    "- MaxPooling2D: 2×2\n",
    "- Conv2D: 64 filters, 3×3 kernel, ReLU activation\n",
    "- MaxPooling2D: 2×2\n",
    "- Flatten\n",
    "- Dense: 128 neurons, ReLU activation\n",
    "- Dropout: 0.25 (keep during inference for MC Dropout)\n",
    "- Output layer: 10 neurons with softmax\n",
    "- Optimizer: SGD with momentum = 0.9, learning rate = 0.01\n",
    "- Loss: sparse categorical crossentropy\n",
    "- epochs = 15, batch size = 32\n",
    "- For MC Dropout:\n",
    "- Enable dropout during inference\n",
    "- Average predictions over 20 stochastic forward passes\n",
    "- Compute the epistemic uncertainty as the predictive variance across passes\n",
    "\n",
    "Q6.1 Report the plain test accuracy of the CNN trained with SGD (no MC Dropout).\n",
    "\n",
    "Q6.2 Report the MC Dropout-enhanced accuracy (averaging 20 stochastic predictions).\n",
    "\n",
    "Q6.3 Compute the average epistemic uncertainty (mean predictive variance) across all test samples. Report it as a deterministic number rounded to 3 decimal places\n",
    "\n",
    "\n",
    "# Imports, seed and data loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebc95e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import numpy as np, random\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_full = datasets.SVHN('./data', split='train', download=True, transform=transform)\n",
    "test_full  = datasets.SVHN('./data', split='test',  download=True, transform=transform)\n",
    "\n",
    "train_ds = Subset(train_full, range(2000))\n",
    "test_ds  = Subset(test_full,  range(500))\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e324f976",
   "metadata": {},
   "source": [
    "# CNN definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd4d3faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 2.2439\n",
      "Epoch 2/15, Loss: 2.2325\n",
      "Epoch 3/15, Loss: 2.2279\n",
      "Epoch 4/15, Loss: 2.2260\n",
      "Epoch 5/15, Loss: 2.2227\n",
      "Epoch 6/15, Loss: 2.2199\n",
      "Epoch 7/15, Loss: 2.2183\n",
      "Epoch 8/15, Loss: 2.2142\n",
      "Epoch 9/15, Loss: 2.2064\n",
      "Epoch 10/15, Loss: 2.1948\n",
      "Epoch 11/15, Loss: 2.1812\n",
      "Epoch 12/15, Loss: 2.1639\n",
      "Epoch 13/15, Loss: 2.1220\n",
      "Epoch 14/15, Loss: 2.0645\n",
      "Epoch 15/15, Loss: 1.9714\n",
      "\n",
      "Plain Test Accuracy: 29.20%\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Q6.1 — Train CNN with SGD\n",
    "# ------------------------------\n",
    "class CNN_SVHN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*8*8, 128), nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.features(x))\n",
    "\n",
    "# Model, optimizer, loss\n",
    "model = CNN_SVHN().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "epochs = 15\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for x, y in train_dl:\n",
    "        x, y = x.to(device), y.to(device).long()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_dl):.4f}\")\n",
    "\n",
    "# Plain test accuracy\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for x, y in test_dl:\n",
    "        x, y = x.to(device), y.to(device).long()\n",
    "        preds = model(x).argmax(1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "plain_acc = 100 * correct / total\n",
    "print(f\"\\nPlain Test Accuracy: {plain_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d1e2c0",
   "metadata": {},
   "source": [
    "### Q6.1 \n",
    "The accuracy is low because the model doesn’t have enough data or depth to really learn the complex patterns in the SVHN images. It’s only trained on 2000 samples, and the network is pretty simple. Also, using plain SGD makes learning slower, so it doesn’t reach good performance within just 15 epochs. ( i think WIP)\n",
    "\n",
    "### Q6.2 Report the MC Dropout-enhanced accuracy (averaging 20 stochastic predictions).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f825375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MC Dropout-enhanced Accuracy: 29.20%\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Q6.2 — MC Dropout Accuracy\n",
    "# ------------------------------\n",
    "def mc_dropout_predictions(model, x, passes=20):\n",
    "    \"\"\"MC Dropout predictions: keep dropout active during inference.\"\"\"\n",
    "    model.train()  # ENABLE dropout\n",
    "    preds = []\n",
    "    for _ in range(passes):\n",
    "        preds.append(torch.softmax(model(x), dim=1).unsqueeze(0))\n",
    "    return torch.cat(preds, dim=0)  # [passes, batch, classes]\n",
    "\n",
    "correct_mc, total_mc = 0, 0\n",
    "with torch.no_grad():\n",
    "    for x, y in test_dl:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        preds_mc = mc_dropout_predictions(model, x, passes=20)\n",
    "        mean_preds = preds_mc.mean(0)\n",
    "        final_preds = mean_preds.argmax(1)\n",
    "        correct_mc += (final_preds == y).sum().item()\n",
    "        total_mc += y.size(0)\n",
    "\n",
    "mc_acc = 100 * correct_mc / total_mc\n",
    "print(f\"MC Dropout-enhanced Accuracy: {mc_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da085a8e",
   "metadata": {},
   "source": [
    "## Q6.3 Compute the average epistemic uncertainty (mean predictive variance) across all test samples. Report it as a deterministic number rounded to 3 decimal places\n",
    "\n",
    "### Whats epistemic uncertainty?\n",
    "Epistemic uncertainty is the uncertainty in a model’s predictions due to lack of knowledge or limited training data. It reflects how unsure the model is about its predictions and can be reduced by providing more data or improving the model. \n",
    "\n",
    "In practice, MC Dropout estimates it by measuring variability in predictions across multiple stochastic forward passes. High epistemic uncertainty means the model is less confident, while low uncertainty means it is more confident in its outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c8ca06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Epistemic Uncertainty: 0.000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "epistemic_vars = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, _ in test_dl:\n",
    "        x = x.to(device)\n",
    "        preds_mc = mc_dropout_predictions(model, x, passes=20)  # [20, batch, 10]\n",
    "        # variance per sample over stochastic passes\n",
    "        var_preds = preds_mc.var(0).mean(1)  # mean over classes\n",
    "        epistemic_vars.extend(var_preds.cpu().numpy())\n",
    "\n",
    "avg_epistemic_uncertainty = np.mean(epistemic_vars)\n",
    "print(f\"Average Epistemic Uncertainty: {avg_epistemic_uncertainty:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01af09e",
   "metadata": {},
   "source": [
    "Gonna be honest. i dont how why its rolling with a 0% uncertainty. This still presists when changing the pass amount. Might have to fix this tomorrow :/ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
